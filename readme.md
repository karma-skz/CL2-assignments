If juniors who have the same assignments stumble upon this, you're welcome. Note that these solutions are not entirely correct, they are provided as a base you can improve upon.

A few quick notes:
- For Naive Bayes models, ensure the features you choose are mutually exclusive where required (i got that wrong but am too lazy to go back and fix it sorry).
- Large datasets have been removed from this repo because of github size constraints. check the assignment writeups (my roll no was xxxxxxx012)

Below is a short summary of the contents of each folder in this repository:

- A1: word length analysis (zipf's law and pearson coefficient)
- A2: language classification based on n-gram counts (using naive bayes)
- A3: a complete naive bayes exercise showing variations like binarization, removing stop words, etc.
- A4: different methods of anaphora resolution
- A5: extractive text summarisation. `my LLM baseline is flawed because i didnt prompt it not to give me an abstractive summary :(`
- A6: implement methods for word sense disambiguation including LESK algorithm. the 2nd part taught about manual annotation of semantic role labels like PropBank and FrameNet tagsets. (I used a tool called inception, which i have written about in the part2 report)
- A7: worked on frequency based embeddings (using singular value decomposition and co-occurence matrices). kinda as an introduction to vector embeddings

contact me if u need anything else, or open an issue idk